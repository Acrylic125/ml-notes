{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Lecture 2 - Regression\n",
    "\n",
    "## Representing Supervised Learning\n",
    "We can represent supervised learning like so:\n",
    "<!-- ![Supervised Learning](img/supervised-learning.png) -->\n",
    "\n",
    "## Regression Hypothesis\n",
    "The regression hypothesis function can be represented like so, where there are j input features:\n",
    "$$\n",
    "h(x) = \\theta_0 + \\theta_1x_1 + ... + \\theta_jx_j\n",
    "$$\n",
    "\n",
    "We can generalize this like so:\n",
    "$$\n",
    "h(x) = \\sum \\limits _{j=0} ^{n} \\theta_jx_j\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_0$ = 1\n",
    "\n",
    "We can represent our input variables like so:\n",
    "$$\n",
    "\\vec{x} = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "... \\\\\n",
    "x_j\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "We can represent our parameters like so:\n",
    "$$\n",
    "\\vec{\\theta} = \\begin{bmatrix}\n",
    "\\theta_1 \\\\\n",
    "... \\\\\n",
    "\\theta_j\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "### Terminology\n",
    "- $x_j$ is the input (Feature j) variable.\n",
    "- $y$ is the output variable.\n",
    "- $m$ is the number of training examples.\n",
    "- $\\theta$ are the parameters.\n",
    "- $(x, y)$ is a training example.\n",
    "- $(x^{(i)}, y^{(i)})$ is the i-th training example.\n",
    "\n",
    "__Note__: The superscript (i) is used to denote the i-th training example.\n",
    "\n",
    "## Cost Function\n",
    "The cost function is average of the squared differences between the predicted value and the actual value. It is represented like so:\n",
    "\n",
    "In linear regression, we want to minimize the cost function which is represented like so:\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum \\limits _{i=1} ^m ((y)^{(i)} - h(x^{(i)}))^2\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "The cost function is used to measure how accurate our hypothesis function is. The closer the cost function is to 0, the more accurate our hypothesis function is.\n",
    "\n",
    "1. $((y)^{(i)} - h(x^{(i)}))^2$ is the squared error for the i-th training example.\n",
    "2. $\\sum \\limits _{i=1} ^m$, we then iterate through all the training sets and sum it all up\n",
    "3. We multiply that sum by $\\frac{1}{m}$ to get the average of the total squared errors. We divide by 2 to make the derivative calculations easier. \n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "Gradient descent is an algorithm that minimizes the cost function, $J(\\theta)$, by iteratively updating the parameters, $\\theta$, by checking the gradient.\n",
    "\n",
    "We can compute the gradient descent like so:\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function to minimize\n",
    "def f(x):\n",
    "    return x**2 + 10*np.sin(x)\n",
    "\n",
    "# Define the derivative of the function\n",
    "def grad_f(x):\n",
    "    return 2*x + 10*np.cos(x)\n",
    "\n",
    "# Initialize the starting point\n",
    "x_0 = 3\n",
    "\n",
    "# Perform gradient descent\n",
    "learning_rate = 0.1\n",
    "x_list = [x_0]\n",
    "for i in range(50):\n",
    "    x_list.append(x_list[-1] - learning_rate*grad_f(x_list[-1]))\n",
    "\n",
    "# Plot the function and the progress of gradient descent\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = f(x)\n",
    "plt.plot(x, y, label='f(x)')\n",
    "plt.scatter(np.array(x_list), f(np.array(x_list)), c='r', label='Gradient Descent')\n",
    "# plt.scatter(x_list, f(x_list), c='r', label='Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4b8989ac986c7ea796f7af66d08683dc62d8031d90bf6de42b490c99542a32e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
