{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Lecture 2 - Regression\n",
    "\n",
    "## Representing Supervised Learning\n",
    "We can represent supervised learning like so:\n",
    "![Supervised Learning](img/supervised-learning.png)\n",
    "\n",
    "## Regression Hypothesis\n",
    "The regression hypothesis function is a function that will be used to guess an output for the passed in features. It can be represented like so, where there are j input features:\n",
    "$$\n",
    "h(x) = \\theta_0 + \\theta_1x_1 + ... + \\theta_jx_j\n",
    "$$\n",
    "\n",
    "We can generalize this like so:\n",
    "$$\n",
    "h(x) = \\sum \\limits _{j=0} ^{n} \\theta_jx_j\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_0$ = 1\n",
    "\n",
    "We can represent our input variables like so:\n",
    "$$\n",
    "\\vec{x} = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "... \\\\\n",
    "x_j\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "We can represent our parameters like so:\n",
    "$$\n",
    "\\vec{\\theta} = \\begin{bmatrix}\n",
    "\\theta_1 \\\\\n",
    "... \\\\\n",
    "\\theta_j\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "### Terminology\n",
    "- $x_j$ is the input (Feature j) variable.\n",
    "- $y$ is the output variable.\n",
    "- $n$ is the number of features.\n",
    "- $m$ is the number of training examples.\n",
    "- $\\theta$ are the parameters.\n",
    "- $(x, y)$ is a training example.\n",
    "- $(x^{(i)}, y^{(i)})$ is the i-th training example.\n",
    "\n",
    "__Note__: The superscript (i) is used to denote the i-th training example.\n",
    "\n",
    "## Cost Function\n",
    "The cost function is average of the squared differences between the predicted value and the actual value. It is represented like so:\n",
    "\n",
    "In linear regression, we want to minimize the cost function which is represented like so:\n",
    "$$\n",
    "Total Cost = \\frac{1}{2} \\sum \\limits _{i=1} ^m ((y)^{(i)} - h(x^{(i)}))^2 \\\\ \n",
    "J(\\theta) = \\frac{1}{2m} Total Cost \\\\ \n",
    "J(\\theta) = \\frac{1}{2m} \\sum \\limits _{i=1} ^m ((y)^{(i)} - h(x^{(i)}))^2\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "The cost function is used to measure how accurate our hypothesis function is. The closer the cost function is to 0, the more accurate our hypothesis function is.\n",
    "\n",
    "1. $((y)^{(i)} - h(x^{(i)}))^2$ is the squared error for the i-th training example.\n",
    "2. $\\sum \\limits _{i=1} ^m$, we then iterate through all the training sets and sum it all up then multiply by $\\frac{1}{2}$ to get the total cost.\n",
    "3. We multiply that sum by $\\frac{1}{m}$ to get the average of all the costs.\n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "A local minimum is like a 'valley', where the cost function is at its lowest point. The goal of gradient descent is to find this local minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function to minimize\n",
    "def f(x):\n",
    "    return x**2 + 10*np.sin(x)\n",
    "\n",
    "# Initialize the starting point\n",
    "x_0 = 3\n",
    "\n",
    "# Plot the function and the progress of gradient descent\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = f(x)\n",
    "\n",
    "# d/dx = 0 : https://www.wolframalpha.com/input?i=2x+%2B+10cos%28x%29+%3D+0\n",
    "# Minimum points: -1.30644, 3.83747\n",
    "\n",
    "min_pts = [-1.30644, 3.83747]\n",
    "\n",
    "plt.plot(x, y, label='f(x)')\n",
    "plt.scatter(np.array(min_pts), \n",
    "            f(np.array(min_pts)), \n",
    "            c='r', \n",
    "            label='Miminum points')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The results shown above are the local minimums for the function, $f(x) = x^2 + 10sin(x)$. Here, we can find these values using the aforementioned strategy but it is not so possible in higher dimensions. Instead, we can use gradient descent to find these values.\n",
    "\n",
    "The goal of gradient descent is to find values for $\\theta$ that minimize the cost function, $J(\\theta)$, so as to lower the \"error\" of our hypothesis function.\n",
    "\n",
    "We can find these 'vallies' by doing the following iteratively until we find a local minimum:\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) $$\n",
    "\n",
    "__Derivation:__\n",
    "We can simplify the above equation by solving for the derrivative.\n",
    "Recall, \n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum \\limits _{i=1} ^m ((y)^{(i)} - h(x^{(i)}))^2$$\n",
    "where $i$ is the i-th training example. Let's first deal with 1 training example. For now, we will include the $\\frac{1}{2}$ in the cost function to simplify our calculations.\n",
    "$$\\frac{1}{2} ((y)^{(i)} - h(x^{(i)}))^2$$\n",
    "Let's find the derrivative of the above equation with respect to $\\theta_j$.\n",
    "$$= \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2} (y - h(x))^2$$\n",
    "Using chain rule, we can simplify the above equation like so:\n",
    "$$= \\frac{\\partial}{\\partial \\theta_j} (\\frac{1}{2} * 2 (y - h(x))^{2-1} \\frac{\\partial}{\\partial \\theta_j} (y - h(x)))$$\n",
    "$$= (y - h(x)) \\frac{\\partial}{\\partial \\theta_j} (y - h(x))$$\n",
    "$$= (y - h(x)) \\frac{\\partial}{\\partial \\theta_j} (y - \\sum \\limits _{k=0} ^n \\theta_kx_k)$$\n",
    "$j$ is the j-th parameter, so we can assumine that $0 \\leq j \\leq n$.\n",
    "$$= (y - h(x)) \\frac{\\partial}{\\partial \\theta_j} (y - (... + \\theta_jx_j + ...))$$\n",
    "Finally, we derive the underived part. Notice, we are taking the partial derrivative with respect to $\\theta_j$, so we can ignore all the terms that do not contain $\\theta_j$ since they will be 0.\n",
    "$$= (y - h(x)) x_j$$\n",
    "\n",
    "Now, we can generalize the above equation for all the training examples.\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum \\limits _{i=1} ^m ((y)^{(i)} - h(x^{(i)}))x_j^{(i)}$$\n",
    "\n",
    "### Explanation\n",
    "The idea here is to update each parameter in $\\theta$ by subtracting the partial derivative of $J(\\theta)$ with respect to $\\theta_j$ multiplied by $\\alpha$.\n",
    "1. We first initialize $\\theta$ to some values like ${\\theta} = \\vec{0}$.\n",
    "2. We keep updating $\\theta$ until we find a local minimum.\n",
    "    - In each iteration, we update each parameter in $\\theta$ by subtracting $\\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$.\n",
    "    - $\\alpha$ is the learning rate (A multiplier that determines how much should each iteration move by).\n",
    "    - $\\frac{\\partial}{\\partial \\theta_j} J(\\theta)$ gives us the direction and how much the update should move for the j-th parameter.\n",
    "\n",
    "__Why do we subtract by the derrivative?__\n",
    "The derrivative tells us how much parameter, $\\theta_j$ needs to move given some change, $\\triangle \\theta_j$ (steepness) for the current position of the cost function at $\\theta$. This which gives us an idea of how 'close' the point is to a local minimum. This is because the minimum points have a gradient of $0$ which means if our the graident of current position is closer to $0$, we can say the point is nearby a local minimum. Thus, the closer we are to a local minimum, the smaller the distance we need to move. This way, we do not overshoot the local minimum.\n",
    "\n",
    "A property of derrivatives is that the direction of the movement will approach a local maximum if we move in the direction of $+\\triangle \\theta_j$ and conversely, a local minimum if we move by $-\\triangle \\theta_j$. This is why we subtract by the derrivative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function to minimize\n",
    "def f(x):\n",
    "    return x**2 + 10*np.sin(x)\n",
    "\n",
    "# Define the derivative of the function\n",
    "def grad_f(x):\n",
    "    return 2*x + 10*np.cos(x)\n",
    "\n",
    "# Initialize the starting point\n",
    "x_0 = 3\n",
    "\n",
    "# Perform gradient descent\n",
    "learning_rate = 0.1\n",
    "x_list = [x_0]\n",
    "for i in range(50):\n",
    "    x_list.append(x_list[-1] - learning_rate*grad_f(x_list[-1]))\n",
    "\n",
    "# Plot the function and the progress of gradient descent\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = f(x)\n",
    "plt.plot(x, y, label='f(x)')\n",
    "plt.scatter(np.array(x_list), f(np.array(x_list)), c='r', label='Gradient Descent')\n",
    "# plt.scatter(x_list, f(x_list), c='r', label='Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4b8989ac986c7ea796f7af66d08683dc62d8031d90bf6de42b490c99542a32e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
